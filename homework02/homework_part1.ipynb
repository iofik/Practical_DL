{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Yandex DataSphere Kernel",
      "language": "python"
    },
    "language_info": {
      "file_extension": ".py",
      "version": "3.7.7",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      },
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "notebookId": "bb380ffe-1071-4186-986c-0ac43d344a0c",
    "colab": {
      "name": "homework_part1.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "h4jkubmprqph396kkzha9e",
        "id": "UMgkmjoYgpaU"
      },
      "source": [
        "# Homework part I\n",
        "\n",
        "The first problem set contains basic tasks in pytorch.\n",
        "\n",
        "__Note:__ Instead of doing this part of homework, you can prove your skills otherwise:\n",
        "* A commit to pytorch or pytorch-based repos will do;\n",
        "* Fully implemented seminar assignment in tensorflow or theano will do;\n",
        "* Your own project in pytorch that is developed to a state in which a normal human can understand and appreciate what it does."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellId": "hlx142siqy73qnmurugg6p",
        "trusted": true,
        "id": "_zyjWtqtgpaW"
      },
      "source": [
        "#!L\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import torch, torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "zb3mxd9m35dbut30ow3omf",
        "id": "ptru6hQdgpaW"
      },
      "source": [
        "### Task I - tensormancy\n",
        "\n",
        "![img](https://media.giphy.com/media/3o751UMCYtSrRAFRFC/giphy.gif)\n",
        "\n",
        "When dealing with more complex stuff like neural network, it's best if you use tensors the way samurai uses his sword. \n",
        "\n",
        "\n",
        "__1.1 the cannabola__ \n",
        "[_disclaimer_](https://gist.githubusercontent.com/justheuristic/e2c1fa28ca02670cabc42cacf3902796/raw/fd3d935cef63a01b85ed2790b5c11c370245cbd7/stddisclaimer.h)\n",
        "\n",
        "Let's write another function, this time in polar coordinates:\n",
        "$$\\rho(\\theta) = (1 + 0.9 \\cdot cos (8 \\cdot \\theta) ) \\cdot (1 + 0.1 \\cdot cos(24 \\cdot \\theta)) \\cdot (0.9 + 0.05 \\cdot cos(200 \\cdot \\theta)) \\cdot (1 + sin(\\theta))$$\n",
        "\n",
        "\n",
        "Then convert it into cartesian coordinates ([howto](http://www.mathsisfun.com/polar-cartesian-coordinates.html)) and plot the results.\n",
        "\n",
        "Use torch tensors only: no lists, loops, numpy arrays, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellId": "63tmtp95mfr8zny5t1m7d7",
        "trusted": true,
        "id": "2UOtW3XggpaX"
      },
      "source": [
        "#!L\n",
        "theta = torch.linspace(- np.pi, np.pi, steps=1000)\n",
        "\n",
        "# compute rho(theta) as per formula above\n",
        "rho = (1 + 0.9 * torch.cos(8 * theta)) \\\n",
        "    * (1 + 0.1 * torch.cos(24 * theta)) \\\n",
        "    * (0.9 + 0.05 * torch.cos(200 * theta)) \\\n",
        "    * (1 + torch.sin(theta))\n",
        "\n",
        "# Now convert polar (rho, theta) pairs into cartesian (x,y) to plot them.\n",
        "x = rho * torch.cos(theta)\n",
        "y = rho * torch.sin(theta)\n",
        "\n",
        "\n",
        "plt.figure(figsize=[6,6])\n",
        "plt.fill(x.numpy(), y.numpy(), color='green')\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "m2zxz949f5qpsrww44wk",
        "id": "gc0oLv21gpaX"
      },
      "source": [
        "### Task II: the game of life\n",
        "\n",
        "Now it's time for you to make something more challenging. We'll implement Conway's [Game of Life](http://web.stanford.edu/~cdebs/GameOfLife/) in _pure pytorch_. \n",
        "\n",
        "While this is still a toy task, implementing game of life this way has one cool benefit: __you'll be able to run it on GPU! __ Indeed, what could be a better use of your gpu than simulating game of life on 1M/1M grids?\n",
        "\n",
        "![img](https://cdn.tutsplus.com/gamedev/authors/legacy/Stephane%20Beniak/2012/09/11/Preview_Image.png)\n",
        "If you've skipped the url above out of sloth, here's the game of life:\n",
        "* You have a 2D grid of cells, where each cell is \"alive\"(1) or \"dead\"(0)\n",
        "* Any living cell that has 2 or 3 neighbors survives, else it dies [0,1 or 4+ neighbors]\n",
        "* Any cell with exactly 3 neighbors becomes alive (if it was dead)\n",
        "\n",
        "For this task, you are given a reference numpy implementation that you must convert to pytorch.\n",
        "_[numpy code inspired by: https://github.com/rougier/numpy-100]_\n",
        "\n",
        "\n",
        "__Note:__ You can find convolution in `torch.nn.functional.conv2d(Z,filters)`. Note that it has a different input format. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellId": "c8zcotrb7cuxvp5365pk3k",
        "trusted": true,
        "id": "aXZMFuA7gpaY"
      },
      "source": [
        "#!L\n",
        "from scipy.signal import correlate2d as conv2d\n",
        "\n",
        "def np_update(Z):\n",
        "    # Count neighbours with convolution\n",
        "    filters = np.array([[1,1,1],\n",
        "                        [1,0,1],\n",
        "                        [1,1,1]])\n",
        "    \n",
        "    N = conv2d(Z,filters,mode='same')\n",
        "    \n",
        "    # Apply rules\n",
        "    birth = (N==3) & (Z==0)\n",
        "    survive = ((N==2) | (N==3)) & (Z==1)\n",
        "    \n",
        "    Z[:] = birth | survive\n",
        "    return Z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellId": "uotp1vmmzh6b13v16ghyd",
        "trusted": true,
        "id": "tna7ECxTgpaY"
      },
      "source": [
        "#!L\n",
        "def torch_update(Z):\n",
        "    \"\"\"\n",
        "    Implement an update function that does to Z exactly the same as np_update.\n",
        "    :param Z: torch.FloatTensor of shape [height,width] containing 0s(dead) an 1s(alive)\n",
        "    :returns: torch.FloatTensor Z after updates.\n",
        "    \n",
        "    You can opt to create new tensor or change Z inplace.\n",
        "    \"\"\"\n",
        "    filters = torch.FloatTensor([[1,1,1],\n",
        "                                 [1,0,1],\n",
        "                                 [1,1,1]])\n",
        "    \n",
        "    Z = Z.reshape(torch.Size([1,1])+Z.shape)\n",
        "    filters = filters.reshape(torch.Size([1,1])+filters.shape)\n",
        "    \n",
        "    N = torch.nn.functional.conv2d(Z, filters, padding=1)\n",
        "    \n",
        "    # Apply rules\n",
        "    birth = (N==3) & (Z==0)\n",
        "    survive = ((N==2) | (N==3)) & (Z==1)\n",
        "    \n",
        "    Z = birth | survive\n",
        "    return (Z+0.).reshape(Z.shape[2:])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellId": "bdjcrg5r4klgc63y833iko",
        "trusted": true,
        "id": "g3guxNV1gpaY"
      },
      "source": [
        "#!L\n",
        "#initial frame\n",
        "Z_numpy = np.random.choice([0,1],p=(0.5,0.5),size=(100,100))\n",
        "Z = torch.from_numpy(Z_numpy).type(torch.FloatTensor)\n",
        "\n",
        "#your debug polygon :)\n",
        "Z_new = torch_update(Z.clone())\n",
        "\n",
        "#tests\n",
        "Z_reference = np_update(Z_numpy.copy())\n",
        "assert np.all(Z_new.numpy() == Z_reference), \"your pytorch implementation doesn't match np_update. Look into Z and np_update(ZZ) to investigate.\"\n",
        "print(\"Well done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellId": "r0o9rikxj53d5v8h3mz4r",
        "trusted": true,
        "id": "qbTa7LYFgpaZ"
      },
      "source": [
        "#!L\n",
        "%matplotlib notebook\n",
        "plt.ion()\n",
        "\n",
        "#initialize game field\n",
        "Z = np.random.choice([0,1],size=(100,100))\n",
        "Z = torch.from_numpy(Z).type(torch.FloatTensor)\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "fig.show()\n",
        "\n",
        "for _ in range(100):\n",
        "    \n",
        "    #update\n",
        "    Z = torch_update(Z)\n",
        "    \n",
        "    #re-draw image\n",
        "    ax.clear()\n",
        "    ax.imshow(Z.numpy(),cmap='gray')\n",
        "    fig.canvas.draw()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellId": "mwj9qklh3vbma1e2u1irl",
        "trusted": true,
        "id": "zEGLa57TgpaZ"
      },
      "source": [
        "#!L\n",
        "#Some fun setups for your amusement\n",
        "\n",
        "#parallel stripes\n",
        "Z = np.arange(100)%2 + np.zeros([100,100])\n",
        "#with a small imperfection\n",
        "Z[48:52,50]=1\n",
        "\n",
        "Z = torch.from_numpy(Z).type(torch.FloatTensor)\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "fig.show()\n",
        "\n",
        "for _ in range(100):\n",
        "    Z = torch_update(Z)\n",
        "    ax.clear()\n",
        "    ax.imshow(Z.numpy(),cmap='gray')\n",
        "    fig.canvas.draw()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "bz8hofx4ah6yly383d75ih",
        "id": "q-FPxCy6gpaa"
      },
      "source": [
        "More fun with Game of Life: [video](https://www.youtube.com/watch?v=C2vgICfQawE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "9fftxzcnnzlqhtitzypkdp",
        "id": "s16fnOadgpab"
      },
      "source": [
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### Task III: Going deeper\n",
        "<img src=\"http://download.gamezone.com/uploads/image/data/1190338/article_post_width_a88.jpg\" width=360>\n",
        "Your third trial is to build your first neural network [almost] from scratch and pure torch.\n",
        "\n",
        "This time you will solve yet another digit recognition problem, but at a greater scale\n",
        "* 10 different letters\n",
        "* 20k samples\n",
        "\n",
        "We want you to build a network that reaches at least 80% accuracy and has at least 2 linear layers in it. Naturally, it should be nonlinear to beat logistic regression. You can implement it with either \n",
        "\n",
        "\n",
        "With 10 classes you will need to use __Softmax__ at the top instead of sigmoid and train for __categorical crossentropy__  (see [here](https://www.kaggle.com/wiki/LogLoss)).  Write your own loss or use `torch.nn.functional.nll_loss`. Just make sure you understand what it accepts as an input.\n",
        "\n",
        "Note that you are not required to build 152-layer monsters here. A 2-layer (one hidden, one output) neural network should already give you an edge over logistic regression.\n",
        "\n",
        "\n",
        "__[bonus kudos]__\n",
        "If you've already beaten logistic regression with a two-layer net, but enthusiasm still ain't gone, you can try improving the test accuracy even further! It should be possible to reach 90% without convnets.\n",
        "\n",
        "__SPOILERS!__\n",
        "At the end of the notebook you will find a few tips and frequent errors. \n",
        "If you feel confident enogh, just start coding right away and get there ~~if~~ once you need to untangle yourself. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "cellId": "400hh7pcv48libce8uhc6b",
        "trusted": true,
        "id": "iZ20-5m3gpab"
      },
      "source": [
        "#!L\n",
        "from notmnist import load_notmnist\n",
        "X_train, y_train, X_test, y_test = load_notmnist(letters='ABCDEFGHIJ')\n",
        "X_train, X_test = X_train.reshape([-1, 784]), X_test.reshape([-1, 784])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellId": "eti17w8s17aigr8s7jkl0s",
        "trusted": true,
        "id": "lgpTahnKgpac"
      },
      "source": [
        "#!L\n",
        "%matplotlib inline\n",
        "plt.figure(figsize=[12,4])\n",
        "for i in range(20):\n",
        "    plt.subplot(2,10,i+1)\n",
        "    plt.imshow(X_train[i].reshape([28,28]))\n",
        "    plt.title(str(y_train[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellId": "plf5xtr48ek7kfg9y3tf",
        "trusted": true,
        "id": "6N-2Zjtmgpac"
      },
      "source": [
        "#!L\n",
        "#< a whole lot of your code > "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellId": "acg3axxegkqvq6i6sy7jw",
        "id": "HyYMcwq1gpac"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "nzlf55u49j822e35jmrm4",
        "id": "AOrridDzgpac"
      },
      "source": [
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "# SPOILERS!\n",
        "\n",
        "Recommended pipeline\n",
        "\n",
        "* Adapt logistic regression from week2 seminar assignment to classify one letter against others (e.g. A vs the rest)\n",
        "* Generalize it to multiclass logistic regression.\n",
        "  - Either try to remember lecture 0 or google it.\n",
        "  - Instead of weight vector you'll have to use matrix (feature_id x class_id)\n",
        "  - softmax (exp over sum of exps) can implemented manually or as nn.Softmax (layer) F.softmax (function)\n",
        "  - probably better to use STOCHASTIC gradient descent (minibatch) for greater speed\n",
        "    - you can also try momentum/rmsprop/adawhatever\n",
        "    - in which case sample should probably be shuffled (or use random subsamples on each iteration)\n",
        "* Add a hidden layer. Now your logistic regression uses hidden neurons instead of inputs.\n",
        "  - Hidden layer uses the same math as output layer (ex-logistic regression), but uses some nonlinearity (e.g. sigmoid) instead of softmax\n",
        "  - You need to train both layers, not just output layer :)\n",
        "  - __Do not initialize weights with zeros__ (due to symmetry effects). A gaussian noize with small variance will do.\n",
        "  - 50 hidden neurons and a sigmoid nonlinearity will do for a start. Many ways to improve. \n",
        "  - In ideal casae this totals to 2 .dot's, 1 softmax and 1 sigmoid\n",
        "  - __make sure this neural network works better than logistic regression__\n",
        "  \n",
        "* Now's the time to try improving the network. Consider layers (size, neuron count),  nonlinearities, optimization methods, initialization - whatever you want, but please avoid convolutions for now.\n",
        "  \n",
        "* If anything seems wrong, try going through one step of training and printing everything you compute.\n",
        "* If you see NaNs midway through optimization, you can estimate log P(y|x) as via F.log_softmax(layer_before_softmax)\n",
        "\n"
      ]
    }
  ]
}