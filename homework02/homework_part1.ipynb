{"nbformat":4,"nbformat_minor":4,"metadata":{"kernelspec":{"name":"python3","display_name":"Yandex DataSphere Kernel","language":"python"},"language_info":{"file_extension":".py","version":"3.7.7","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3"},"notebookId":"bb380ffe-1071-4186-986c-0ac43d344a0c"},"cells":[{"cell_type":"markdown","source":"# Homework part I\n\nThe first problem set contains basic tasks in pytorch.\n\n__Note:__ Instead of doing this part of homework, you can prove your skills otherwise:\n* A commit to pytorch or pytorch-based repos will do;\n* Fully implemented seminar assignment in tensorflow or theano will do;\n* Your own project in pytorch that is developed to a state in which a normal human can understand and appreciate what it does.","metadata":{"cellId":"h4jkubmprqph396kkzha9e"}},{"cell_type":"code","source":"#!L\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport torch, torch.nn as nn\nimport torch.nn.functional as F\nprint(torch.__version__)","metadata":{"cellId":"hlx142siqy73qnmurugg6p","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task I - tensormancy\n\n![img](https://media.giphy.com/media/3o751UMCYtSrRAFRFC/giphy.gif)\n\nWhen dealing with more complex stuff like neural network, it's best if you use tensors the way samurai uses his sword. \n\n\n__1.1 the cannabola__ \n[_disclaimer_](https://gist.githubusercontent.com/justheuristic/e2c1fa28ca02670cabc42cacf3902796/raw/fd3d935cef63a01b85ed2790b5c11c370245cbd7/stddisclaimer.h)\n\nLet's write another function, this time in polar coordinates:\n$$\\rho(\\theta) = (1 + 0.9 \\cdot cos (8 \\cdot \\theta) ) \\cdot (1 + 0.1 \\cdot cos(24 \\cdot \\theta)) \\cdot (0.9 + 0.05 \\cdot cos(200 \\cdot \\theta)) \\cdot (1 + sin(\\theta))$$\n\n\nThen convert it into cartesian coordinates ([howto](http://www.mathsisfun.com/polar-cartesian-coordinates.html)) and plot the results.\n\nUse torch tensors only: no lists, loops, numpy arrays, etc.","metadata":{"cellId":"zb3mxd9m35dbut30ow3omf"}},{"cell_type":"code","source":"#!L\ntheta = torch.linspace(- np.pi, np.pi, steps=1000)\n\n# compute rho(theta) as per formula above\nrho = (1 + 0.9 * torch.cos(8 * theta)) \\\n    * (1 + 0.1 * torch.cos(24 * theta)) \\\n    * (0.9 + 0.05 * torch.cos(200 * theta)) \\\n    * (1 + torch.sin(theta))\n\n# Now convert polar (rho, theta) pairs into cartesian (x,y) to plot them.\nx = rho * torch.cos(theta)\ny = rho * torch.sin(theta)\n\n\nplt.figure(figsize=[6,6])\nplt.fill(x.numpy(), y.numpy(), color='green')\nplt.grid()","metadata":{"cellId":"63tmtp95mfr8zny5t1m7d7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task II: the game of life\n\nNow it's time for you to make something more challenging. We'll implement Conway's [Game of Life](http://web.stanford.edu/~cdebs/GameOfLife/) in _pure pytorch_. \n\nWhile this is still a toy task, implementing game of life this way has one cool benefit: __you'll be able to run it on GPU! __ Indeed, what could be a better use of your gpu than simulating game of life on 1M/1M grids?\n\n![img](https://cdn.tutsplus.com/gamedev/authors/legacy/Stephane%20Beniak/2012/09/11/Preview_Image.png)\nIf you've skipped the url above out of sloth, here's the game of life:\n* You have a 2D grid of cells, where each cell is \"alive\"(1) or \"dead\"(0)\n* Any living cell that has 2 or 3 neighbors survives, else it dies [0,1 or 4+ neighbors]\n* Any cell with exactly 3 neighbors becomes alive (if it was dead)\n\nFor this task, you are given a reference numpy implementation that you must convert to pytorch.\n_[numpy code inspired by: https://github.com/rougier/numpy-100]_\n\n\n__Note:__ You can find convolution in `torch.nn.functional.conv2d(Z,filters)`. Note that it has a different input format. \n","metadata":{"cellId":"m2zxz949f5qpsrww44wk"}},{"cell_type":"code","source":"#!L\nfrom scipy.signal import correlate2d as conv2d\n\ndef np_update(Z):\n    # Count neighbours with convolution\n    filters = np.array([[1,1,1],\n                        [1,0,1],\n                        [1,1,1]])\n    \n    N = conv2d(Z,filters,mode='same')\n    \n    # Apply rules\n    birth = (N==3) & (Z==0)\n    survive = ((N==2) | (N==3)) & (Z==1)\n    \n    Z[:] = birth | survive\n    return Z","metadata":{"cellId":"c8zcotrb7cuxvp5365pk3k","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!L\ndef torch_update(Z):\n    \"\"\"\n    Implement an update function that does to Z exactly the same as np_update.\n    :param Z: torch.FloatTensor of shape [height,width] containing 0s(dead) an 1s(alive)\n    :returns: torch.FloatTensor Z after updates.\n    \n    You can opt to create new tensor or change Z inplace.\n    \"\"\"\n    filters = torch.FloatTensor([[1,1,1],\n                                 [1,0,1],\n                                 [1,1,1]])\n    \n    Z = Z.reshape(torch.Size([1,1])+Z.shape)\n    filters = filters.reshape(torch.Size([1,1])+filters.shape)\n    \n    N = torch.nn.functional.conv2d(Z, filters, padding=1)\n    \n    # Apply rules\n    birth = (N==3) & (Z==0)\n    survive = ((N==2) | (N==3)) & (Z==1)\n    \n    Z = birth | survive\n    return Z+0\n","metadata":{"cellId":"uotp1vmmzh6b13v16ghyd","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!L\n#initial frame\nZ_numpy = np.random.choice([0,1],p=(0.5,0.5),size=(100,100))\nZ = torch.from_numpy(Z_numpy).type(torch.FloatTensor)\n\n#your debug polygon :)\nZ_new = torch_update(Z.clone())\n\n#tests\nZ_reference = np_update(Z_numpy.copy())\nassert np.all(Z_new.numpy() == Z_reference), \"your pytorch implementation doesn't match np_update. Look into Z and np_update(ZZ) to investigate.\"\nprint(\"Well done!\")","metadata":{"cellId":"bdjcrg5r4klgc63y833iko","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!L\n%matplotlib notebook\nplt.ion()\n\n#initialize game field\nZ = np.random.choice([0,1],size=(100,100))\nZ = torch.from_numpy(Z).type(torch.FloatTensor)\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nfig.show()\n\nfor _ in range(100):\n    \n    #update\n    Z = torch_update(Z)\n    \n    #re-draw image\n    ax.clear()\n    ax.imshow(Z.numpy(),cmap='gray')\n    fig.canvas.draw()\n","metadata":{"cellId":"r0o9rikxj53d5v8h3mz4r","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!L\n#Some fun setups for your amusement\n\n#parallel stripes\nZ = np.arange(100)%2 + np.zeros([100,100])\n#with a small imperfection\nZ[48:52,50]=1\n\nZ = torch.from_numpy(Z).type(torch.FloatTensor)\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nfig.show()\n\nfor _ in range(100):\n    Z = torch_update(Z)\n    ax.clear()\n    ax.imshow(Z.numpy(),cmap='gray')\n    fig.canvas.draw()","metadata":{"cellId":"mwj9qklh3vbma1e2u1irl","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"More fun with Game of Life: [video](https://www.youtube.com/watch?v=C2vgICfQawE)","metadata":{"cellId":"bz8hofx4ah6yly383d75ih"}},{"cell_type":"markdown","source":"```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n\n\n### Task III: Going deeper\n<img src=\"http://download.gamezone.com/uploads/image/data/1190338/article_post_width_a88.jpg\" width=360>\nYour third trial is to build your first neural network [almost] from scratch and pure torch.\n\nThis time you will solve yet another digit recognition problem, but at a greater scale\n* 10 different letters\n* 20k samples\n\nWe want you to build a network that reaches at least 80% accuracy and has at least 2 linear layers in it. Naturally, it should be nonlinear to beat logistic regression. You can implement it with either \n\n\nWith 10 classes you will need to use __Softmax__ at the top instead of sigmoid and train for __categorical crossentropy__  (see [here](https://www.kaggle.com/wiki/LogLoss)).  Write your own loss or use `torch.nn.functional.nll_loss`. Just make sure you understand what it accepts as an input.\n\nNote that you are not required to build 152-layer monsters here. A 2-layer (one hidden, one output) neural network should already give you an edge over logistic regression.\n\n\n__[bonus kudos]__\nIf you've already beaten logistic regression with a two-layer net, but enthusiasm still ain't gone, you can try improving the test accuracy even further! It should be possible to reach 90% without convnets.\n\n__SPOILERS!__\nAt the end of the notebook you will find a few tips and frequent errors. \nIf you feel confident enogh, just start coding right away and get there ~~if~~ once you need to untangle yourself. \n\n","metadata":{"cellId":"9fftxzcnnzlqhtitzypkdp"}},{"cell_type":"code","source":"#!L\nfrom notmnist import load_notmnist\nX_train, y_train, X_test, y_test = load_notmnist(letters='ABCDEFGHIJ')\nX_train, X_test = X_train.reshape([-1, 784]), X_test.reshape([-1, 784])","metadata":{"scrolled":true,"cellId":"400hh7pcv48libce8uhc6b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!L\n%matplotlib inline\nplt.figure(figsize=[12,4])\nfor i in range(20):\n    plt.subplot(2,10,i+1)\n    plt.imshow(X_train[i].reshape([28,28]))\n    plt.title(str(y_train[i]))","metadata":{"cellId":"eti17w8s17aigr8s7jkl0s","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!L\n#< a whole lot of your code > ","metadata":{"cellId":"plf5xtr48ek7kfg9y3tf","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"cellId":"acg3axxegkqvq6i6sy7jw"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n\n# SPOILERS!\n\nRecommended pipeline\n\n* Adapt logistic regression from week2 seminar assignment to classify one letter against others (e.g. A vs the rest)\n* Generalize it to multiclass logistic regression.\n  - Either try to remember lecture 0 or google it.\n  - Instead of weight vector you'll have to use matrix (feature_id x class_id)\n  - softmax (exp over sum of exps) can implemented manually or as nn.Softmax (layer) F.softmax (function)\n  - probably better to use STOCHASTIC gradient descent (minibatch) for greater speed\n    - you can also try momentum/rmsprop/adawhatever\n    - in which case sample should probably be shuffled (or use random subsamples on each iteration)\n* Add a hidden layer. Now your logistic regression uses hidden neurons instead of inputs.\n  - Hidden layer uses the same math as output layer (ex-logistic regression), but uses some nonlinearity (e.g. sigmoid) instead of softmax\n  - You need to train both layers, not just output layer :)\n  - __Do not initialize weights with zeros__ (due to symmetry effects). A gaussian noize with small variance will do.\n  - 50 hidden neurons and a sigmoid nonlinearity will do for a start. Many ways to improve. \n  - In ideal casae this totals to 2 .dot's, 1 softmax and 1 sigmoid\n  - __make sure this neural network works better than logistic regression__\n  \n* Now's the time to try improving the network. Consider layers (size, neuron count),  nonlinearities, optimization methods, initialization - whatever you want, but please avoid convolutions for now.\n  \n* If anything seems wrong, try going through one step of training and printing everything you compute.\n* If you see NaNs midway through optimization, you can estimate log P(y|x) as via F.log_softmax(layer_before_softmax)\n\n","metadata":{"cellId":"nzlf55u49j822e35jmrm4"}}]}