{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework: Basic Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this homework is simple, yet an actual implementation may take some time :). We are going to write an Artificial Neural Network (almost) from scratch. The software design was heavily inspired by [PyTorch](http://pytorch.org) which is the main framework of our course "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework requires sending **multiple** files, please do not forget to include all the files when sending to TA. The list of files:\n",
    "- This notebook\n",
    "- homework_modules.ipynb with all blocks implemented (except maybe `Conv2d` and `MaxPool2d` layers implementation which are part of 'advanced' version of this homework)\n",
    "- homework_differentiation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from time import time, sleep\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from collections import defaultdict\n",
    "from math import prod\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement everything in `Modules.ipynb`. Read all the comments thoughtfully to ease the pain. Please try not to change the prototypes.\n",
    "\n",
    "Do not forget, that each module should return **AND** store `output` and `gradInput`.\n",
    "\n",
    "The typical assumption is that `module.backward` is always executed after `module.forward`,\n",
    "so `output` is stored, this would be useful for `SoftMax`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tech note\n",
    "Prefer using `np.multiply`, `np.add`, `np.divide`, `np.subtract` instead of `*`,`+`,`/`,`-` for better memory handling.\n",
    "\n",
    "Example: suppose you allocated a variable \n",
    "\n",
    "```\n",
    "a = np.zeros(...)\n",
    "```\n",
    "So, instead of\n",
    "```\n",
    "a = b + c  # will be reallocated, GC needed to free\n",
    "``` \n",
    "You can use: \n",
    "```\n",
    "np.add(b,c,out = a) # puts result in `a`\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (re-)load layers\n",
    "%run homework_modules.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this example to debug your code, start with logistic regression and then test other layers. You do not need to change anything here. This code is provided for you to test the layers. Also it is easy to use this code in MNIST task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "N = 500\n",
    "\n",
    "X1 = np.random.randn(N,2) + np.array([2,2])\n",
    "X2 = np.random.randn(N,2) + np.array([-2,-2])\n",
    "\n",
    "Y = np.concatenate([np.ones(N),np.zeros(N)])[:,None]\n",
    "Y = np.hstack([Y, 1-Y])\n",
    "\n",
    "X = np.vstack([X1,X2])\n",
    "plt.scatter(X[:,0],X[:,1], c = Y[:,0], edgecolors= 'none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a **logistic regression** for debugging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Sequential()\n",
    "net.add(Linear(2, 2))\n",
    "net.add(LogSoftMax())\n",
    "\n",
    "criterion = ClassNLLCriterion()\n",
    "\n",
    "# Test something like that then \n",
    "\n",
    "# net = Sequential()\n",
    "# net.add(Linear(2, 4))\n",
    "# net.add(ReLU())\n",
    "# net.add(Linear(4, 2))\n",
    "# net.add(LogSoftMax())\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with batch_size = 1000 to make sure every step lowers the loss, then try stochastic version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer params\n",
    "optimizer_config = {'learning_rate' : 1e-1, 'momentum': 0.9}\n",
    "optimizer_state = {}\n",
    "\n",
    "# Looping params\n",
    "n_epoch = 5\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch generator\n",
    "def get_batches(dataset, batch_size):\n",
    "    X, Y = dataset\n",
    "    n_samples = X.shape[0]\n",
    "        \n",
    "    # Shuffle at the start of epoch\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_idx = indices[start:end]\n",
    "    \n",
    "        yield X[batch_idx], Y[batch_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic training loop. Examine it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(X, Y, net, opt, opt_config, criterion, n_epoch, batch_size):\n",
    "    opt_state = {}\n",
    "    \n",
    "    for i in range(n_epoch):\n",
    "        for x_batch, y_batch in get_batches((X, Y), batch_size):\n",
    "\n",
    "            net.zeroGradParameters()\n",
    "\n",
    "            # Forward\n",
    "            predictions = net.forward(x_batch)\n",
    "            yield criterion.forward(predictions, y_batch)\n",
    "\n",
    "            # Backward\n",
    "            dp = criterion.backward(predictions, y_batch)\n",
    "            net.backward(x_batch, dp)\n",
    "\n",
    "            # Update weights\n",
    "            opt(net.getParameters(), net.getGradParameters(), opt_config, opt_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "\n",
    "for loss in training_loop(X, Y, net=net, opt=sgd_momentum, opt_config=optimizer_config,\n",
    "                          criterion=criterion, n_epoch=n_epoch, batch_size=batch_size):\n",
    "    loss_history.append(loss)\n",
    "\n",
    "    # Visualize\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "        \n",
    "    plt.title(\"Training loss\")\n",
    "    plt.xlabel(\"#iteration\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.plot(loss_history, 'b')\n",
    "    plt.show()\n",
    "    \n",
    "    print('Current loss: %f' % loss)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using old good [MNIST](http://yann.lecun.com/exdb/mnist/) as our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = mnist.load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encode the labels first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here. ################################################\n",
    "def one_hot_encode(y, n_classes):\n",
    "    Y = np.zeros(y.shape + (n_classes,))\n",
    "    Y[range(Y.shape[0]), y] = 1\n",
    "    return Y\n",
    "\n",
    "Y_train = one_hot_encode(y_train, 10)\n",
    "Y_val = one_hot_encode(y_val, 10)\n",
    "Y_test = one_hot_encode(y_test, 10)\n",
    "\n",
    "def flatten(X):\n",
    "    return X.reshape((X.shape[0], prod(X.shape[1:])))\n",
    "\n",
    "X_train_f = flatten(X_train)\n",
    "X_val_f = flatten(X_val)\n",
    "X_test_f = flatten(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Compare** `ReLU`, `ELU`, `LeakyReLU`, `SoftPlus` activation functions. \n",
    "You would better pick the best optimizer params for each of them, but it is overkill for now. Use an architecture of your choice for the comparison.\n",
    "- **Try** inserting `BatchNormalization` (folowed by `ChannelwiseScaling`) between `Linear` module and activation functions.\n",
    "- Plot the losses both from activation functions comparison and `BatchNormalization` comparison on one plot. Please find a scale (log?) when the lines are distinguishable, do not forget about naming the axes, the plot should be goodlooking.\n",
    "- Plot the losses for two networks: one trained by momentum_sgd, another one trained by Adam. Which one performs better?\n",
    "- Hint: good logloss for MNIST should be around 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_net(act_layer, batch_norm=False):\n",
    "    n_in = X_train_f.shape[1]\n",
    "    n_hidden = 256\n",
    "    \n",
    "    net = Sequential()\n",
    "    net.add(Linear(n_in, n_hidden))\n",
    "    if batch_norm:\n",
    "        net.add(BatchNormalization(0.9))\n",
    "        net.add(ChannelwiseScaling(n_hidden))\n",
    "    net.add(act_layer)\n",
    "    net.add(Linear(n_hidden, 10))\n",
    "    net.add(LogSoftMax())\n",
    "    \n",
    "    return net\n",
    "\n",
    "config = {\n",
    "    'ReLU': {\n",
    "        'net': mk_net(ReLU()),\n",
    "        'color': 'b'\n",
    "    },\n",
    "    'ELU': {\n",
    "        'net': mk_net(ELU()),\n",
    "        'color': 'r'\n",
    "    },\n",
    "    'LeakyReLU': {\n",
    "        'net': mk_net(LeakyReLU()),\n",
    "        'color': 'g'\n",
    "    },\n",
    "    'SoftPlus': {\n",
    "        'net': mk_net(SoftPlus()),\n",
    "        'color': 'y'\n",
    "    },\n",
    "}\n",
    "\n",
    "def visualise_history(loss_history):\n",
    "    # Visualize\n",
    "    display.clear_output(wait=True)\n",
    "    f = plt.figure(figsize=(8, 6))\n",
    "\n",
    "    plt.title(\"Training loss\")\n",
    "    plt.xlabel(\"#iteration\")\n",
    "    plt.ylabel(\"log-loss\")\n",
    "    plt.yscale('log')\n",
    "    for name in config:\n",
    "        plt.plot(loss_history[name], config[name]['color'])\n",
    "    plt.show()\n",
    "\n",
    "    for name in config:\n",
    "        print('Current %s loss: %f' % (name, loss_history[name][-1]))\n",
    "\n",
    "\n",
    "def visualise_loss(opt, opt_config):\n",
    "    loss_history = defaultdict(lambda: [])\n",
    "    loops = {name:training_loop(X_train_f, Y_train, net=conf['net'],\n",
    "                                opt=opt, opt_config=opt_config,\n",
    "                                criterion=criterion, n_epoch=n_epoch, batch_size=batch_size)\n",
    "             for name, conf in config.items()}\n",
    "    try:\n",
    "        while True:\n",
    "            for name, loss in loops.items():\n",
    "                loss_history[name].append(next(loss))\n",
    "\n",
    "            if random() > max(batch_size / y_train.shape[0] * 100, 0.01):\n",
    "                continue\n",
    "            \n",
    "            visualise_history(loss_history)\n",
    "\n",
    "    except StopIteration:\n",
    "        pass\n",
    "\n",
    "    visualise_history(loss_history)\n",
    "\n",
    "\n",
    "visualise_loss(sgd_momentum, optimizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_loss(adam_optimizer, {'learning_rate': 1e-3, 'beta1': 0.9, 'beta2':0.999, 'epsilon':1e-8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your personal opinion on the activation functions, think about computation times too. Does `BatchNormalization` help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer goes here. ################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally**, use all your knowledge to build a super cool model on this dataset. Use **dropout** to prevent overfitting, play with **learning rate decay**. You can use **data augmentation** such as rotations, translations to boost your score. Use your knowledge and imagination to train a model. Don't forget to call `training()` and `evaluate()` methods to set desired behaviour of `BatchNormalization` and `Dropout` layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here. ################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print here your accuracy on test set. It should be around 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer goes here. ################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
